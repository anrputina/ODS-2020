{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import json, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "rootPath = '../../'\n",
    "sys.path.insert(0, rootPath+'Architecture/')\n",
    "from groundTruth import GroundTruth\n",
    "from statistics import Statistics\n",
    "from dataManagement import Scaler\n",
    "\n",
    "from ods import Sample, ODS\n",
    "from multiprocessing import Process, Queue\n",
    "np.seterr(invalid='ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import copy\n",
    "from sklearn.metrics import average_precision_score\n",
    "import multiprocessing, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_node(node, rootPath):\n",
    "    features_node = json.loads(open(rootPath+'features_node.json').read())\n",
    "    features_to_use = features_node[node]['DataPlane']+features_node[node]['ControlPlane']\n",
    "    features_to_use = features_to_use+['time']\n",
    "    len(features_to_use)\n",
    "    return features_to_use\n",
    "\n",
    "def get_interval_scores(dataset, times, scores_np, output_series):\n",
    "    st = Statistics(dataset, rootPath)\n",
    "    st.gt.loadGroundTruthBGP_testbed()\n",
    "    eventsList = st.gt.df.to_dict('records')\n",
    "    \n",
    "    if dataset == 'BGP_testbed_3':\n",
    "        start =  eventsList.pop(3)\n",
    "        end = eventsList.pop(3)\n",
    "        start['End'] = end['End']\n",
    "        eventsList.append(start)\n",
    "\n",
    "        start = eventsList.pop(3)\n",
    "        middle = eventsList.pop(3)\n",
    "        end =  eventsList.pop(3)\n",
    "\n",
    "        start['End'] = end['End']\n",
    "        eventsList.append(start)        \n",
    "\n",
    "    label = pd.Series(np.zeros(scores_np.shape[0]))\n",
    "    \n",
    "    for event in eventsList:\n",
    "        indexes = times[(times>event['Start']) & (times<event['End'])].index\n",
    "        first_index = indexes[0]\n",
    "        label.drop(indexes, inplace=True)\n",
    "        label[first_index] = 1\n",
    "\n",
    "        output_series.drop(indexes, inplace=True)\n",
    "        output_series[first_index] = scores_np[indexes].max()    \n",
    "\n",
    "    label = label.sort_index().reset_index(drop=True)\n",
    "    output_series = output_series.sort_index().reset_index(drop=True)\n",
    "    \n",
    "    return label, output_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb = 0.125\n",
    "beta = 0.4\n",
    "radius_factor_type = 'dynamic'\n",
    "\n",
    "total_scores_ods = {}\n",
    "\n",
    "scores_df_list = []\n",
    "\n",
    "for dataset in ['BGP_testbed_2', 'BGP_testbed_3', 'BGP_testbed_5', 'BGP_testbed_9', 'BGP_testbed_10']:\n",
    "    total_scores_ods[dataset] = {}\n",
    "    config = json.loads(open(rootPath+'configuration.json').read())['datasets'][dataset] \n",
    "    for node in config['nodes']:\n",
    "        \n",
    "        total_scores_ods[dataset][node]= {}\n",
    "        features_node = get_features_node(node, rootPath)\n",
    "\n",
    "        df = pd.read_csv(rootPath + config['directory']+node+config['filename'],\n",
    "                         low_memory=False, dtype='float64')\\\n",
    "                .dropna()\n",
    "        df = df[features_node]\n",
    "\n",
    "        times = df['time']//1e9\n",
    "        times = times.astype('int')\n",
    "        df.drop(['time'], axis=1, inplace=True)    \n",
    "\n",
    "        scaler = Scaler()\n",
    "        dfNormalized = scaler.normalize(df)    \n",
    "\n",
    "        sampleSkip = 30\n",
    "\n",
    "        bufferDF = dfNormalized[0:sampleSkip]\n",
    "        testDF = dfNormalized[sampleSkip:]\n",
    "\n",
    "        ods = ODS(lamb = lamb,\\\n",
    "                    epsilon = radius_factor_type,\\\n",
    "                    beta = beta,\\\n",
    "                    mu = 'auto',\\\n",
    "                    startingBuffer = bufferDF.values,\n",
    "                    tp = 'auto')\n",
    "        ods.runInitialization()\n",
    "\n",
    "        outputCurrentNode = []\n",
    "        output_score = []\n",
    "        for sampleNumber in range(len(testDF)):\n",
    "            sample = testDF.iloc[sampleNumber]\n",
    "            label, score = ods.runOnNewSample(Sample(sample.values, times.iloc[sampleNumber]))\n",
    "            outputCurrentNode.append(label)\n",
    "            output_score.append(score)        \n",
    "        output_score = np.array(output_score)\n",
    "        output_score = (np.where(np.isinf(output_score), output_score[~np.isinf(output_score)].max(), output_score))\n",
    "\n",
    "        scores_ods = np.append(np.zeros(sampleSkip), output_score)\n",
    "        output_ods = pd.Series(copy.deepcopy(scores_ods))\n",
    "\n",
    "        label, interval_scores = get_interval_scores(dataset, times, scores_ods, output_ods)\n",
    "\n",
    "        total_scores_ods[dataset][node]['label'] = label\n",
    "        total_scores_ods[dataset][node]['scores'] = interval_scores\n",
    "        \n",
    "        scores_df_list.append({\n",
    "            'dataset': dataset,\n",
    "            'node': node,\n",
    "            'label': label,\n",
    "            'scores': interval_scores\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save ODS results to pickle\n",
    "\"\"\"\n",
    "pd.DataFrame(scores_df_list).to_pickle('ods_scores.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RRCF in multiprocessing\n",
    "\"\"\"\n",
    "\n",
    "def get_rrcf_tasks():\n",
    "    tasks = []\n",
    "    for dataset in ['BGP_testbed_5', 'BGP_testbed_9', 'BGP_testbed_10']:\n",
    "        config = json.loads(open(rootPath+'configuration.json').read())['datasets'][dataset] \n",
    "        for node in config['nodes']:\n",
    "            for test_id in range(10):\n",
    "                tasks.append({\n",
    "                    'dataset':dataset,\n",
    "                    'node':node,\n",
    "                    'test_id': test_id\n",
    "                })\n",
    "    return tasks\n",
    "\n",
    "def run_rrcf(task):\n",
    "    import rrcf\n",
    "    \n",
    "    dataset = task['dataset']\n",
    "    node = task['node']\n",
    "    \n",
    "    features_node = get_features_node(node, rootPath)\n",
    "    config = json.loads(open(rootPath+'configuration.json').read())['datasets'][dataset]\n",
    "    df = pd.read_csv(rootPath + config['directory']+node+config['filename'],\n",
    "                     low_memory=False, dtype='float64')\\\n",
    "            .dropna()\n",
    "    df = df[features_node]\n",
    "\n",
    "    times = df['time']//1e9\n",
    "    times = times.astype('int')\n",
    "    df.drop(['time'], axis=1, inplace=True)    \n",
    "\n",
    "    scaler = Scaler()\n",
    "    points = scaler.normalize(df).values  \n",
    "    \n",
    "    tree_size = 95\n",
    "    num_trees = 100\n",
    "    \n",
    "    # Create a forest of empty trees\n",
    "    forest = []\n",
    "    for _ in range(num_trees):\n",
    "        tree = rrcf.RCTree()\n",
    "        forest.append(tree)\n",
    "    # Create a dict to store anomaly score of each point\n",
    "    avg_codisp = {}\n",
    "\n",
    "    for index, point in enumerate(points):\n",
    "        # For each tree in the forest...\n",
    "        for tree in forest:\n",
    "            # If tree is above permitted size, drop the oldest point (FIFO)\n",
    "            if len(tree.leaves) > tree_size:\n",
    "                tree.forget_point(index - tree_size)\n",
    "            # Insert the new point into the tree\n",
    "            tree.insert_point(point, index=index)\n",
    "            # Compute codisp on the new point and take the average among all trees\n",
    "            if not index in avg_codisp:\n",
    "                avg_codisp[index] = 0\n",
    "            avg_codisp[index] += tree.codisp(index) / num_trees\n",
    "\n",
    "    scores = np.array(list(avg_codisp.values()))\n",
    "    scores[:sampleSkip] = 0\n",
    "    \n",
    "    scores_rrcf = scores\n",
    "    output_rrcf = pd.Series(copy.deepcopy(scores_rrcf))\n",
    "\n",
    "    label, interval_scores = get_interval_scores(dataset, times, scores_rrcf, output_rrcf)\n",
    "\n",
    "    task['label'] = label.values\n",
    "    task['scores'] = interval_scores.values    \n",
    "    \n",
    "    return task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330\n"
     ]
    }
   ],
   "source": [
    "MAX_PROCESSES = 30\n",
    "p = multiprocessing.Pool(processes=MAX_PROCESSES)\n",
    "\n",
    "tasks = get_rrcf_tasks()\n",
    "print(len(tasks))\n",
    "\n",
    "results = p.map_async(run_rrcf, tasks)\n",
    "\n",
    "p.close()\n",
    "p.join()\n",
    "\n",
    "totalRes = []\n",
    "\n",
    "for r in results.get():\n",
    "    totalRes.append(r)\n",
    "\n",
    "rrcf_res = pd.DataFrame(totalRes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save RRCF results to pickle\n",
    "\"\"\"\n",
    "rrcf_res.to_pickle('rrcf_scores.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
