{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['BGP_testbed_2','BGP_testbed_3','BGP_testbed_5','BGP_testbed_9', 'BGP_testbed_10']\n",
    "\n",
    "config = json.loads(open('../../configuration.json').read())\n",
    "\n",
    "methods = ['dbscan', 'lof', 'wdbscan', 'exactSTORM', 'COD', 'rrcf', 'ods']\n",
    "features_set = 'ALL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_results_dataset(methods):\n",
    "    res = {}\n",
    "    \n",
    "    for method in methods:\n",
    "        print(method)\n",
    "        res[method] = pd.read_json('../../Experiments/'+method+'/Results/'+method+'_results_test_'+features_set+'.json').astype(float)\n",
    "    return res\n",
    "\n",
    "def read_results_general_scores(methods):\n",
    "    res = {}\n",
    "    for method in methods:\n",
    "        res[method] = json.loads(open('../../Experiments/'+method+'/Results/'+method+'_general_results_test_'+features_set+'.json').read())\n",
    "    return res\n",
    "\n",
    "def read_results_general_scores_tune(methods):\n",
    "    res = {}\n",
    "    for method in methods:\n",
    "        res[method] = json.loads(open('../../Experiments/'+method+'/Results/'+method+'_general_results_tune_'+features_set+'.json').read())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_methods_result(methods_data, dataset_name):\n",
    "    \n",
    "    result = {}\n",
    "    result['dataset_name'] = dataset_name\n",
    "    result['scores'] = []\n",
    "    \n",
    "    for method in methods:\n",
    "        method_scores = []\n",
    "        method_scores.append(method)\n",
    "        method_scores.append(round(methods_data[method].loc['avg_prec', dataset_name],2))\n",
    "        method_scores.append(round(methods_data[method].loc['ci_prec_low', dataset_name],2))\n",
    "        \n",
    "        method_scores.append(round(methods_data[method].loc['avg_rec', dataset_name],2))\n",
    "        method_scores.append(round(methods_data[method].loc['ci_rec_low', dataset_name],2))\n",
    "        \n",
    "        method_scores.append(round(methods_data[method].loc['avg_fscore', dataset_name],2))\n",
    "        method_scores.append(round(methods_data[method].loc['ci_fscore_low', dataset_name],2))\n",
    "                \n",
    "        result['scores'].append(method_scores)\n",
    "        \n",
    "    return result\n",
    "\n",
    "def add_total(methods_data, dataset_name, current_result, metrics = 'PRF'):\n",
    "    \n",
    "    result = {}\n",
    "    result['dataset_name'] = dataset_name\n",
    "    result['scores'] = []\n",
    "    \n",
    "    for method in methods:\n",
    "        method_scores = []\n",
    "        method_scores.append(method)\n",
    "        \n",
    "        if metrics == 'PRF':\n",
    "            \n",
    "            method_scores.append(round(methods_data[method]['precision'],4))\n",
    "            method_scores.append(round(methods_data[method]['ci_precision'],4))     \n",
    "\n",
    "            method_scores.append(round(methods_data[method]['recall'],4))\n",
    "            method_scores.append(round(methods_data[method]['ci_recall'],4))  \n",
    "\n",
    "            method_scores.append(round(methods_data[method]['fscore'],4))\n",
    "            method_scores.append(round(methods_data[method]['ci_fscore'],4))\n",
    "            \n",
    "        elif metrics == 'AMI':\n",
    "                \n",
    "            method_scores.append(round(methods_data[method]['ACC'],3))\n",
    "            method_scores.append(round(methods_data[method]['ci_ACC'],3))       \n",
    "\n",
    "            method_scores.append(round(methods_data[method]['MK'],3))\n",
    "            method_scores.append(round(methods_data[method]['ci_MK'],3))               \n",
    "\n",
    "            method_scores.append(round(methods_data[method]['BM'],3))\n",
    "            method_scores.append(round(methods_data[method]['ci_BM'],3))\n",
    "        \n",
    "        else:\n",
    "            print('UNKNOWN metrics')\n",
    "                        \n",
    "        result['scores'].append(method_scores)\n",
    "        \n",
    "    current_result.append(result)\n",
    "\n",
    "def get_results(methods_data, metrics):\n",
    "    res = []\n",
    "#     for dataset in datasets:\n",
    "#         res.append(get_dataset_methods_result(methods_data, dataset))\n",
    "        \n",
    "    methods_data_test = read_results_general_scores(methods)\n",
    "    methods_data_tune = read_results_general_scores_tune(methods)\n",
    "\n",
    "    add_total(methods_data_test, 'test', res, metrics)\n",
    "    add_total(methods_data_tune, 'tune', res, metrics)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbscan\n",
      "lof\n",
      "wdbscan\n",
      "exactSTORM\n",
      "COD\n",
      "rrcf\n",
      "ods\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'dataset_name': 'test',\n",
       "  'scores': [['dbscan', 0.9128, 0.0584, 0.5924, 0.0817, 0.7931, 0.0604],\n",
       "   ['lof', 0.7101, 0.1044, 0.7758, 0.0768, 0.7016, 0.0918],\n",
       "   ['wdbscan', 1.0, nan, 0.351, 0.0697, 0.7098, 0.0526],\n",
       "   ['exactSTORM', 1.0, nan, 0.3813, 0.0779, 0.7277, 0.0573],\n",
       "   ['COD', 0.7332, 0.0852, 0.3606, 0.0746, 0.5855, 0.0497],\n",
       "   ['rrcf', 0.8791, 0.0648, 0.7596, 0.0787, 0.8284, 0.0573],\n",
       "   ['ods', 0.9575, 0.0314, 0.5419, 0.0785, 0.7867, 0.0511]]},\n",
       " {'dataset_name': 'tune',\n",
       "  'scores': [['dbscan', 0.9852, 0.0218, 0.8839, 0.0489, 0.9595, 0.0188],\n",
       "   ['lof', 0.9885, 0.0167, 0.8912, 0.0469, 0.9647, 0.0168],\n",
       "   ['wdbscan', 1.0, nan, 0.6964, 0.0691, 0.9106, 0.0266],\n",
       "   ['exactSTORM', 1.0, nan, 0.7037, 0.0677, 0.9141, 0.0241],\n",
       "   ['COD', 0.9015, 0.0631, 0.7151, 0.0647, 0.8488, 0.0556],\n",
       "   ['rrcf', 0.9607, 0.0463, 0.8369, 0.0593, 0.9266, 0.0407],\n",
       "   ['ods', 1.0, nan, 0.6347, 0.0806, 0.8814, 0.0351]]}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Please choose the metrics to parse. \n",
    "\n",
    "metrics == 'PRF' to produce Precision-Recall-Fscore results (Fig. 8)\n",
    "metrics = 'AMI' produce Accuracy-Markedness-Informedness results (Fig. 9)\n",
    "\"\"\"\n",
    "\n",
    "metrics = 'PRF'\n",
    "# metrics = 'AMI'\n",
    "\n",
    "methods_data_dataset = read_results_dataset(methods)\n",
    "res = get_results(methods_data_dataset, metrics = metrics)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('total_results_'+features_set+'_'+metrics+'.json', 'w') as f:\n",
    "    json.dump(res, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbscan\n",
      "lof\n",
      "wdbscan\n",
      "exactSTORM\n",
      "COD\n",
      "rrcf\n",
      "ods\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'dataset_name': 'test',\n",
       "  'scores': [['dbscan', 0.986, 0.003, 0.901, 0.059, 0.59, 0.082],\n",
       "   ['lof', 0.979, 0.007, 0.703, 0.105, 0.762, 0.079],\n",
       "   ['wdbscan', 0.982, 0.004, 0.983, 0.003, 0.351, 0.07],\n",
       "   ['exactSTORM', 0.983, 0.004, 0.984, 0.003, 0.381, 0.078],\n",
       "   ['COD', 0.978, 0.004, 0.718, 0.083, 0.356, 0.074],\n",
       "   ['rrcf', 0.988, 0.004, 0.871, 0.065, 0.756, 0.079],\n",
       "   ['ods', 0.986, 0.003, 0.944, 0.031, 0.541, 0.078]]},\n",
       " {'dataset_name': 'tune',\n",
       "  'scores': [['dbscan', 0.996, 0.002, 0.981, 0.021, 0.883, 0.049],\n",
       "   ['lof', 0.996, 0.002, 0.985, 0.017, 0.891, 0.047],\n",
       "   ['wdbscan', 0.99, 0.003, 0.99, 0.003, 0.696, 0.069],\n",
       "   ['exactSTORM', 0.99, 0.003, 0.99, 0.003, 0.704, 0.068],\n",
       "   ['COD', 0.988, 0.003, 0.892, 0.064, 0.712, 0.065],\n",
       "   ['rrcf', 0.993, 0.003, 0.954, 0.046, 0.835, 0.059],\n",
       "   ['ods', 0.988, 0.003, 0.987, 0.003, 0.635, 0.081]]}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Please choose the metrics to parse. \n",
    "\n",
    "metrics == 'PRF' to produce Precision-Recall-Fscore results (Fig. 8)\n",
    "metrics = 'AMI' produce Accuracy-Markedness-Informedness results (Fig. 9)\n",
    "\"\"\"\n",
    "\n",
    "# metrics = 'PRF'\n",
    "metrics = 'AMI'\n",
    "\n",
    "methods_data_dataset = read_results_dataset(methods)\n",
    "res = get_results(methods_data_dataset, metrics = metrics)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('total_results_'+features_set+'_'+metrics+'.json', 'w') as f:\n",
    "    json.dump(res, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
