{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import json, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "rootPath = '../../'\n",
    "sys.path.insert(0, rootPath+'Architecture/')\n",
    "from groundTruth import GroundTruth\n",
    "from statistics import Statistics\n",
    "from dataManagement import Scaler\n",
    "\n",
    "from outlierdenstream import Sample, OutlierDenStream\n",
    "from multiprocessing import Process, Queue\n",
    "np.seterr(invalid='ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "# import rrcf\n",
    "import copy\n",
    "from sklearn.metrics import average_precision_score\n",
    "import multiprocessing, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius_factor_type = 'incremental_radius'\n",
    "maxProcesses = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_node(node, rootPath):\n",
    "    features_node = json.loads(open(rootPath+'features_node.json').read())\n",
    "    features_to_use = features_node[node]['DataPlane']+features_node[node]['ControlPlane']\n",
    "    features_to_use = features_to_use+['time']\n",
    "    len(features_to_use)\n",
    "    return features_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interval_scores(dataset, times, scores_np, output_series):\n",
    "    st = Statistics(dataset, rootPath)\n",
    "    st.gt.loadGroundTruthBGP_testbed()\n",
    "    eventsList = st.gt.df.to_dict('records')\n",
    "    \n",
    "    if dataset == 'BGP_testbed_3':\n",
    "        start =  eventsList.pop(3)\n",
    "        end = eventsList.pop(3)\n",
    "        start['End'] = end['End']\n",
    "        eventsList.append(start)\n",
    "\n",
    "        start = eventsList.pop(3)\n",
    "        middle = eventsList.pop(3)\n",
    "        end =  eventsList.pop(3)\n",
    "\n",
    "        start['End'] = end['End']\n",
    "        eventsList.append(start)        \n",
    "\n",
    "    label = pd.Series(np.zeros(scores_np.shape[0]))\n",
    "    \n",
    "    for event in eventsList:\n",
    "        indexes = times[(times>event['Start']) & (times<event['End'])].index\n",
    "        first_index = indexes[0]\n",
    "        label.drop(indexes, inplace=True)\n",
    "        label[first_index] = 1\n",
    "\n",
    "        output_series.drop(indexes, inplace=True)\n",
    "        output_series[first_index] = scores_np[indexes].max()    \n",
    "\n",
    "    label = label.sort_index().reset_index(drop=True)\n",
    "    output_series = output_series.sort_index().reset_index(drop=True)\n",
    "    \n",
    "    return label, output_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb = 0.125\n",
    "beta = 0.4\n",
    "\n",
    "total_scores_ods = {}\n",
    "\n",
    "scores_df_list = []\n",
    "\n",
    "for dataset in ['BGP_testbed_2', 'BGP_testbed_3', 'BGP_testbed_5', 'BGP_testbed_9', 'BGP_testbed_10']:\n",
    "    total_scores_ods[dataset] = {}\n",
    "    config = json.loads(open(rootPath+'configuration.json').read())['datasets'][dataset] \n",
    "    for node in config['nodes']:\n",
    "        \n",
    "        total_scores_ods[dataset][node]= {}\n",
    "        features_node = get_features_node(node, rootPath)\n",
    "\n",
    "        df = pd.read_csv(rootPath + config['directory']+node+config['filename'],\n",
    "                         low_memory=False, dtype='float64')\\\n",
    "                .dropna()\n",
    "        df = df[features_node]\n",
    "\n",
    "        times = df['time']//1e9\n",
    "        times = times.astype('int')\n",
    "        df.drop(['time'], axis=1, inplace=True)    \n",
    "\n",
    "        scaler = Scaler()\n",
    "        dfNormalized = scaler.normalize(df)    \n",
    "\n",
    "        sampleSkip = 30\n",
    "\n",
    "        bufferDF = dfNormalized[0:sampleSkip]\n",
    "        testDF = dfNormalized[sampleSkip:]\n",
    "\n",
    "        ods = OutlierDenStream(lamb = lamb,\\\n",
    "                    epsilon = radius_factor_type,\\\n",
    "                    beta = beta,\\\n",
    "                    mu = 'auto',\\\n",
    "                    startingBuffer = bufferDF.values,\n",
    "                    tp = 'auto')\n",
    "        ods.runInitialization()\n",
    "\n",
    "        outputCurrentNode = []\n",
    "        output_score = []\n",
    "        for sampleNumber in range(len(testDF)):\n",
    "            sample = testDF.iloc[sampleNumber]\n",
    "            label, score = ods.runOnNewSample(Sample(sample.values, times.iloc[sampleNumber]))\n",
    "            outputCurrentNode.append(label)\n",
    "            output_score.append(score)        \n",
    "        output_score = np.array(output_score)\n",
    "        output_score = (np.where(np.isinf(output_score), output_score[~np.isinf(output_score)].max(), output_score))\n",
    "\n",
    "        scores_ods = np.append(np.zeros(sampleSkip), output_score)\n",
    "        output_ods = pd.Series(copy.deepcopy(scores_ods))\n",
    "\n",
    "        label, interval_scores = get_interval_scores(dataset, times, scores_ods, output_ods)\n",
    "\n",
    "        total_scores_ods[dataset][node]['label'] = label\n",
    "        total_scores_ods[dataset][node]['scores'] = interval_scores\n",
    "        \n",
    "        scores_df_list.append({\n",
    "            'dataset': dataset,\n",
    "            'node': node,\n",
    "            'label': label,\n",
    "            'scores': interval_scores\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(scores_df_list).to_pickle('ods_scores.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(output_score)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset = 'BGP_testbed_3'\n",
    "node = 'spine1'\n",
    "# for dataset in ['BGP_testbed_2', 'BGP_testbed_3']:\n",
    "config = json.loads(open(rootPath+'configuration.json').read())['datasets'][dataset] \n",
    "features_node = get_features_node(node, rootPath)\n",
    "\n",
    "lamb = 0.125\n",
    "beta = 0.4\n",
    "\n",
    "df = pd.read_csv(rootPath + config['directory']+node+config['filename'],\n",
    "                 low_memory=False, dtype='float64')\\\n",
    "        .dropna()\n",
    "df = df[features_node]\n",
    "\n",
    "times = df['time']//1e9\n",
    "times = times.astype('int')\n",
    "df.drop(['time'], axis=1, inplace=True)    \n",
    "\n",
    "scaler = Scaler()\n",
    "dfNormalized = scaler.normalize(df)    \n",
    "\n",
    "sampleSkip = 30\n",
    "\n",
    "bufferDF = dfNormalized[0:sampleSkip]\n",
    "testDF = dfNormalized[sampleSkip:]\n",
    "\n",
    "ods = OutlierDenStream(lamb = lamb,\\\n",
    "            epsilon = radius_factor_type,\\\n",
    "            beta = beta,\\\n",
    "            mu = 'auto',\\\n",
    "            startingBuffer = bufferDF.values,\n",
    "            tp = 'auto')\n",
    "ods.runInitialization()\n",
    "\n",
    "outputCurrentNode = []\n",
    "output_score = []\n",
    "for sampleNumber in range(len(testDF)):\n",
    "    sample = testDF.iloc[sampleNumber]\n",
    "    label, score = ods.runOnNewSample(Sample(sample.values, times.iloc[sampleNumber]))\n",
    "    outputCurrentNode.append(label)\n",
    "    output_score.append(score)\n",
    "    \n",
    "output_score = np.array(output_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rrcf_tasks():\n",
    "    tasks = []\n",
    "    for dataset in ['BGP_testbed_2', 'BGP_testbed_3', 'BGP_testbed_5', 'BGP_testbed_9', 'BGP_testbed_10']:\n",
    "        config = json.loads(open(rootPath+'configuration.json').read())['datasets'][dataset] \n",
    "        for node in config['nodes']:\n",
    "            for test_id in range(10):\n",
    "                tasks.append({\n",
    "                    'dataset':dataset,\n",
    "                    'node':node,\n",
    "                    'test_id': test_id\n",
    "                })\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output_score = (np.where(np.isinf(output_score), output_score[~np.isinf(output_score)].max(), output_score))\n",
    "plt.plot(output_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rrcf(task):\n",
    "    import rrcf\n",
    "    \n",
    "    dataset = task['dataset']\n",
    "    node = task['node']\n",
    "    \n",
    "    features_node = get_features_node(node, rootPath)\n",
    "    config = json.loads(open(rootPath+'configuration.json').read())['datasets'][dataset]\n",
    "    df = pd.read_csv(rootPath + config['directory']+node+config['filename'],\n",
    "                     low_memory=False, dtype='float64')\\\n",
    "            .dropna()\n",
    "    df = df[features_node]\n",
    "\n",
    "    times = df['time']//1e9\n",
    "    times = times.astype('int')\n",
    "    df.drop(['time'], axis=1, inplace=True)    \n",
    "\n",
    "    scaler = Scaler()\n",
    "    points = scaler.normalize(df).values  \n",
    "    \n",
    "    tree_size = 95\n",
    "    num_trees = 100\n",
    "    \n",
    "    # Create a forest of empty trees\n",
    "    forest = []\n",
    "    for _ in range(num_trees):\n",
    "        tree = rrcf.RCTree()\n",
    "        forest.append(tree)\n",
    "    # Create a dict to store anomaly score of each point\n",
    "    avg_codisp = {}\n",
    "\n",
    "    for index, point in enumerate(points):\n",
    "        # For each tree in the forest...\n",
    "        for tree in forest:\n",
    "            # If tree is above permitted size, drop the oldest point (FIFO)\n",
    "            if len(tree.leaves) > tree_size:\n",
    "                tree.forget_point(index - tree_size)\n",
    "            # Insert the new point into the tree\n",
    "            tree.insert_point(point, index=index)\n",
    "            # Compute codisp on the new point and take the average among all trees\n",
    "            if not index in avg_codisp:\n",
    "                avg_codisp[index] = 0\n",
    "            avg_codisp[index] += tree.codisp(index) / num_trees\n",
    "\n",
    "    scores = np.array(list(avg_codisp.values()))\n",
    "    scores[:30] = 0\n",
    "    \n",
    "    scores_rrcf = scores\n",
    "    output_rrcf = pd.Series(copy.deepcopy(scores_rrcf))\n",
    "\n",
    "    label, interval_scores = get_interval_scores(dataset, times, scores_rrcf, output_rrcf)\n",
    "\n",
    "    task['label'] = label.values\n",
    "    task['scores'] = interval_scores.values    \n",
    "    \n",
    "    return task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550\n"
     ]
    }
   ],
   "source": [
    "MAX_PROCESSES = 30\n",
    "p = multiprocessing.Pool(processes=MAX_PROCESSES)\n",
    "\n",
    "tasks = get_rrcf_tasks()\n",
    "print(len(tasks))\n",
    "\n",
    "results = p.map_async(run_rrcf, tasks)\n",
    "\n",
    "p.close()\n",
    "p.join()\n",
    "\n",
    "totalRes = []\n",
    "\n",
    "for r in results.get():\n",
    "    totalRes.append(r)\n",
    "\n",
    "rrcf_res = pd.DataFrame(totalRes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrcf_res.to_pickle('rrcf_scores.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lof_tasks():\n",
    "    tasks = []\n",
    "    for dataset in ['BGP_testbed_2', 'BGP_testbed_3', 'BGP_testbed_5', 'BGP_testbed_9', 'BGP_testbed_10']:\n",
    "        config = json.loads(open(rootPath+'configuration.json').read())['datasets'][dataset] \n",
    "        for node in config['nodes']:\n",
    "            tasks.append({\n",
    "                'dataset':dataset,\n",
    "                'node':node\n",
    "            })\n",
    "    return tasks    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "def run_lof(task):\n",
    "    dataset = task['dataset']\n",
    "    node = task['node']\n",
    "    \n",
    "    features_node = get_features_node(node, rootPath)\n",
    "    config = json.loads(open(rootPath+'configuration.json').read())['datasets'][dataset]\n",
    "    df = pd.read_csv(rootPath + config['directory']+node+config['filename'],\n",
    "                     low_memory=False, dtype='float64')\\\n",
    "            .dropna()\n",
    "    df = df[features_node]\n",
    "\n",
    "    times = df['time']//1e9\n",
    "    times = times.astype('int')\n",
    "    df.drop(['time'], axis=1, inplace=True)    \n",
    "\n",
    "    scaler = Scaler()\n",
    "    points = scaler.normalize(df).values     \n",
    "    \n",
    "    lof = LocalOutlierFactor(n_neighbors=24,\n",
    "                            leaf_size=30)\n",
    "\n",
    "    lof.fit(points)\n",
    "    \n",
    "    scores = np.negative(lof.negative_outlier_factor_)\n",
    "    \n",
    "    scores[:30] = 0\n",
    "    \n",
    "    scores_lof = scores\n",
    "    output_lof = pd.Series(copy.deepcopy(scores_lof))\n",
    "\n",
    "    label, interval_scores = get_interval_scores(dataset, times, scores_lof, output_lof)\n",
    "\n",
    "    task['label'] = label.values\n",
    "    task['scores'] = interval_scores.values  \n",
    "    \n",
    "    return task\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "MAX_PROCESSES = 30\n",
    "p = multiprocessing.Pool(processes=MAX_PROCESSES)\n",
    "\n",
    "tasks = get_lof_tasks()\n",
    "print(len(tasks))\n",
    "\n",
    "results = p.map_async(run_lof, tasks)\n",
    "\n",
    "p.close()\n",
    "p.join()\n",
    "\n",
    "totalRes = []\n",
    "\n",
    "for r in results.get():\n",
    "    totalRes.append(r)\n",
    "\n",
    "lof_res = pd.DataFrame(totalRes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_res.to_pickle('lof_scores.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...\n",
       "1      [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...\n",
       "2      [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...\n",
       "3      [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...\n",
       "4      [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...\n",
       "                             ...                        \n",
       "545    [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...\n",
       "546    [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...\n",
       "547    [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...\n",
       "548    [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...\n",
       "549    [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ...\n",
       "Name: label, Length: 550, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scores = run_rrcf(dfNormalized.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2)\n",
    "ax[0].plot(scores)\n",
    "ax[1].plot(np.append(np.zeros(30), output_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = Statistics(dataset, rootPath)\n",
    "st.gt.loadGroundTruthBGP_testbed()\n",
    "eventsList = st.gt.df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "label = pd.Series(np.zeros(dfNormalized.shape[0]))\n",
    "output_rrcf = pd.Series(copy.deepcopy(scores))\n",
    "scores_ods = np.append(np.zeros(30), output_score)\n",
    "output_ods = pd.Series(copy.deepcopy(scores_ods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in eventsList:\n",
    "    indexes = times[(times>event['Start']) & (times<event['End'])].index\n",
    "    first_index = indexes[0]\n",
    "    label.drop(indexes, inplace=True)\n",
    "    label[first_index] = 1\n",
    "    \n",
    "    output_rrcf.drop(indexes, inplace=True)\n",
    "    output_rrcf[first_index] = scores[indexes].max()\n",
    "    \n",
    "    output_ods.drop(indexes, inplace=True)\n",
    "    output_ods[first_index] = scores_ods[indexes].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = label.sort_index().reset_index(drop=True)\n",
    "output_rrcf = output_rrcf.sort_index().reset_index(drop=True)\n",
    "output_ods = output_ods.sort_index().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision_score(label, output_rrcf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision_score(label, output_ods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(label, output_rrcf)\n",
    "ax.plot(recall, precision, label='rrcf')\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(label, output_ods)\n",
    "ax.plot(recall, precision, label='ods')\n",
    "\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
